{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["FZAXt1DZcqYW"],"mount_file_id":"1zmFktj5PHYse2H2IQCukzYqOLn745vzP","authorship_tag":"ABX9TyPmjeQxEBMV554RzdO1m/dP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#My Special AI Agent"],"metadata":{"id":"aqsgdahyXp5J"}},{"cell_type":"markdown","source":["## General Setup"],"metadata":{"id":"ifuTFwD3v3hj"}},{"cell_type":"markdown","source":["### Mount Google Drive\n","In the below code block, you will mount your Google Drive to the Google colaboratory notebook.\n","\n","You will be asked to go to a URL in a new browser window, sign in to your Google account, and allow Google Drive File Stream to access your Google Drive files.\n","\n","Technically, you could mount the Google Drive through the files portion of the Google Colab UI, but this method is for the purpose of being explicit."],"metadata":{"id":"iYTSudV5xVjy"}},{"cell_type":"code","source":["# Unnecessary to run if Google Drive is already mounted (Check files in left tab)\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"lFgmlI4Ov15F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Download Necessary Libraries/Packages"],"metadata":{"id":"lXUH_a0Qx-rb"}},{"cell_type":"markdown","source":["To load and use your pretrained model, you will need to have the correct libraries installed. First, the script will perform a typical update and upgrade to the packages. Then, the following libraries/packages will be installed.\n","\n","Below are the necessary libraries/packages:\n","*   git: For using git and GitHub\n","*   git lfs: For using large files inside the git system\n","*   transformers: For using transformer based models\n","*   torch: For using PyTorch\n","*   huggingface_hub: For logging into Hugging Face"],"metadata":{"id":"RWyBaoZhyMYk"}},{"cell_type":"code","source":["!sudo apt update -y\n","!sudo apt upgrade -y --allow-change-held-packages"],"metadata":{"id":"_wmwgyXA8II3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!sudo apt install git\n","#!git lfs install\n","\n","!pip install transformers datasets\n","#!pip install torch\n","!pip install huggingface_hub\n","!pip install bitsandbytes\n","!pip install accelerate\n","!pip install evaluate\n","#!wget -nv -O- https://lambdalabs.com/install-lambda-stack.sh | sh -"],"metadata":{"id":"5IV6GRW_wfLD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import necessary libraries"],"metadata":{"id":"i7H43m9-XcdQ"}},{"cell_type":"code","source":["import os\n","import torch\n","import datasets\n","from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig\n","from transformers import AutoTokenizer, AutoModel, pipeline, Conversation\n","from transformers.pipelines.pt_utils import KeyDataset\n","from tqdm.auto import tqdm\n","\n","CACHE_DIR = \"/content/drive/MyDrive/Colab_Notebooks/AI_Models/.cache/huggingface/hub/\"\n","os.environ[\"HF_HOME\"] = CACHE_DIR"],"metadata":{"id":"ImqDRyVrXhUT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## AI Model Setup"],"metadata":{"id":"7CMN6yogXuBA"}},{"cell_type":"markdown","source":["### Login to Hugging Face"],"metadata":{"id":"2GdbfymM8jIy"}},{"cell_type":"markdown","source":["**Logging in to Hugging Face may be unnecessary. Return later to verify.**"],"metadata":{"id":"dznWc31W8rJY"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login"],"metadata":{"id":"WObaK1ljXF4o","colab":{"base_uri":"https://localhost:8080/","height":314},"executionInfo":{"status":"error","timestamp":1686789338855,"user_tz":360,"elapsed":222,"user":{"displayName":"Joe Pollock","userId":"00229025514646850072"}},"outputId":"42b90599-2135-4795-c861-6c6e04714755"},"execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-178b6f8aab0b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook_login\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'huggingface_hub'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"wx2_SQbgrUWI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AI Model Folder"],"metadata":{"id":"E-2ygRTn84tg"}},{"cell_type":"markdown","source":["Navigate to AI Model stored in Google Drive."],"metadata":{"id":"hI74Qz2sYPMI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9jBHl_GxXhwu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686336139992,"user_tz":360,"elapsed":1614,"user":{"displayName":"Joe Pollock","userId":"00229025514646850072"}},"outputId":"40efcd17-6bca-4834-ff59-191dab3a3370"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/AI Models/WizardLM-13B-Uncensored\n"]}],"source":["%cd /content/drive/MyDrive/Colab_Notebooks/AI_Models/WizardLM-13B-Uncensored/"]},{"cell_type":"markdown","source":["Check the current directory using the following script:"],"metadata":{"id":"jNA1N6ZRAzOZ"}},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"YMAs7AqAA2i7","executionInfo":{"status":"ok","timestamp":1686336141291,"user_tz":360,"elapsed":35,"user":{"displayName":"Joe Pollock","userId":"00229025514646850072"}},"outputId":"58b09cc0-467d-4f91-abb3-e03f7b4d13b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/AI Models/WizardLM-13B-Uncensored'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Make sure the output shows the same directory as the %cd command directly above it."],"metadata":{"id":"ll8lOjNVCO5l"}},{"cell_type":"markdown","source":["#### GPU Usage"],"metadata":{"id":"melHnHi5WUuK"}},{"cell_type":"markdown","source":["Below is a command you can run to check the specs of the GPU being run in this instance."],"metadata":{"id":"lLknoErLHiOy"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"hJuWOSBaHd1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.is_available                                     # check for cuda"],"metadata":{"id":"GkTkAxnpWpGh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.get_device_name(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"570GWoL5Y8pQ","executionInfo":{"status":"ok","timestamp":1686343078962,"user_tz":360,"elapsed":468,"user":{"displayName":"Joe Pollock","userId":"00229025514646850072"}},"outputId":"5f4b161a-5131-40e8-e7ed-7b3dcff64f08"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")"],"metadata":{"id":"-hnzR-ylZ7Mo","executionInfo":{"status":"ok","timestamp":1686789425441,"user_tz":360,"elapsed":300,"user":{"displayName":"Joe Pollock","userId":"00229025514646850072"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Load the Model & Tokenizer"],"metadata":{"id":"RQ57ydZ39EA0"}},{"cell_type":"markdown","source":["After installing the necessary libraries, you can load the model and tokenizer using the `AutoModel` and `AutoTokenizer` classes from the `transformers` library. These classes can automatically infer the correct model and tokenizer classes to use based on the config.json and tokenizer configuration files."],"metadata":{"id":"z-MMR4cC9JsU"}},{"cell_type":"markdown","source":["Here, '.' is used to indicate the current directory. If your model files are in a different directory, you should replace '.' with the path to that directory."],"metadata":{"id":"teMo-w6a90Td"}},{"cell_type":"markdown","source":["#### AutoTokenizer"],"metadata":{"id":"jssYMWB0IrKM"}},{"cell_type":"markdown","source":["A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split.\n","\n","The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure youâ€™re using the same tokenization rules a model was pretrained with."],"metadata":{"id":"KExCnHNVI2i4"}},{"cell_type":"code","source":["#import torch\n","#from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig\n","\n","# To load a model from memory\n","#PATH = '/content/drive/MyDrive/Colab_Notebooks/AI_Models/WizardLM-13B-Uncensored/'\n","#model = LlamaForCausalLM.from_pretrained(PATH)\n","#tokenizer = LlamaTokenizer.from_pretrained(PATH)\n","\n","#model = AutoModel.from_pretrained(PATH)\n","#model.to(device)\n","#tokenizer = AutoTokenizer.from_pretrained(PATH)\n","\n","# To load a model from Hugging Face\n","model_name = \"databricks/dolly-v2-3b\"\n","question_answerer = pipeline(\"question-answering\", model=model_name, trust_remote_code=True, device_map=0, model_kwargs={'load_in_8bit': True})"],"metadata":{"id":"2RweHwGY9W9a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train the Model\n"],"metadata":{"id":"FZAXt1DZcqYW"}},{"cell_type":"markdown","source":["Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:"],"metadata":{"id":"bRTuIWSicww-"}},{"cell_type":"code","source":["import math\n","eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"],"metadata":{"id":"gOXFRviCcvI9","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"error","timestamp":1686789550206,"user_tz":360,"elapsed":228,"user":{"displayName":"Joe Pollock","userId":"00229025514646850072"}},"outputId":"4610a581-08fa-44f0-e8f8-d66865cb6219"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-75f05755702f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"]}]},{"cell_type":"markdown","source":["##Use the Model"],"metadata":{"id":"lYYTBAMh-AKM"}},{"cell_type":"markdown","source":["Now that the model and tokenizer are loaded, you can use them to generate predictions. The exact way to do this will depend on what kind of model you have, but here's a simple example of how you might generate predictions from a text input:"],"metadata":{"id":"NzM2oNfI-DM4"}},{"cell_type":"markdown","source":["### Provide the Prompt"],"metadata":{"id":"dMvRTq31J7EE"}},{"cell_type":"markdown","source":["In the code snippet below, you will be providing input for the model in the `prompt` variable."],"metadata":{"id":"FUtKFHAkJ-sp"}},{"cell_type":"code","source":["# Encode a text inputs\n","question = \"What do you know?\"\n","context = \"I am a curious first-time tester.\"\n","question_answerer(question=question, context=context)"],"metadata":{"id":"R11FxqgN-H8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The tokenizer returns a dictionary containing:\n","*   input_ids: numerical representations of your tokens\n","*   attention_mask: indicates which tokens should be attended to\n","\n","A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length"],"metadata":{"id":"D-A1zBSxPMo2"}},{"cell_type":"code","source":["outputs = model(**prompt)\n","print(outputs)"],"metadata":{"id":"ZlFBKhwTPAM4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate Output"],"metadata":{"id":"4426gEDkKPUE"}},{"cell_type":"markdown","source":["In the code snippet below, you will be executing the commands for the model to generate output based on the input provided previously."],"metadata":{"id":"T5gx01-IKSF4"}},{"cell_type":"code","source":["# Generate output from the model\n","#output = model(input_ids)\n","generate_ids = model.generate(inputs.input_ids, max_length=30)\n","tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","\n","# The output will be a tuple, with the model's forward pass output as the first item\n","#predictions = output[0]"],"metadata":{"id":"gVlNvOaLKcIc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Save the Model"],"metadata":{"id":"DDZOtnG_LZXi"}},{"cell_type":"markdown","source":["Once your model is fine-tuned, you can save it with its tokenizer using PreTrainedModel.save_pretrained():"],"metadata":{"id":"PLW73s_sLcVF"}},{"cell_type":"code","source":["pt_save_directory = \"./pt_save_pretrained\"\n","tokenizer.save_pretrained(pt_save_directory)\n","pt_model.save_pretrained(pt_save_directory)"],"metadata":{"id":"EExL9RXoN2YZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Miscellaneous"],"metadata":{"id":"LkWiNGaM99x9"}},{"cell_type":"markdown","source":["Below is for training the model with datasets."],"metadata":{"id":"N1gKqIhs8-6e"}},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"json\", data_files=\"my_file.json\")"],"metadata":{"id":"zlMbhcYut7x2"},"execution_count":null,"outputs":[]}]}